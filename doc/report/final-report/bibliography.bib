
@inproceedings{manuel_ai-enhanced_2026,
	title = {{AI}-{Enhanced} {Glasses} for the {Visually} {Impaired}},
	volume = {1463 LNNS},
	isbn = {978-981-96-7513-5},
	doi = {10.1007/978-981-96-7514-2_32},
	abstract = {Smart assistive technologies improve the independence and access to transportation of the vision-impaired people (Alotaibi in WCSNE 2015 Proceedings. Infonomics Society, 2015, [1]). This paper presents AI-enhanced smart glasses designed to empower visually impaired by transforming their interaction with the environment. The system combines cutting-edge technologies to offer a comprehensive assistive solution. A path navigation algorithm, powered by YOLOv8, ensures safe mobility by detecting obstacles and guiding users with audio feedback. Integrated OCR capabilities enable real-time text reading, while an image captioning module provides detailed scene descriptions for enhanced situational awareness. The glasses incorporate a camera and an ultrasonic sensor, delivering robust performance in diverse scenarios, including detecting objects at close proximity where traditional algorithms fall short. With seamless audio output for user interaction, the proposed solution bridges the gap between technology and accessibility, promising independence and confidence for visually impaired individuals. This innovative fusion of AI and sensory inputs defines a new benchmark for assistive devices. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2026.},
	language = {English},
	booktitle = {Lect. {Notes} {Networks} {Syst}.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Manuel, Austine S. and John, Alana Ance and Rafeeq, Hamna and Anas, Thalhah and Thomas, Manoj V.},
	year = {2026},
	note = {Journal Abbreviation: Lect. Notes Networks Syst.},
	keywords = {Image captioning, Navigational assistance, Visually impaired, YOLOv8},
	pages = {397--411},
	file = {Snapshot:/Users/florian/Zotero/storage/UC6KE32Q/105020254364.html:text/html},
}

@inproceedings{loor_guaycha_functional_2026,
	title = {Functional {Prototype} of an {Ultrasonic} {Smart} {Cane} for {Visually} {Impaired} {People}},
	volume = {1429 LNNS},
	isbn = {978-3-031-99338-1},
	doi = {10.1007/978-3-031-99339-8_2},
	abstract = {Visually impaired individuals face significant challenges in navigating urban environments with a high density of obstacles. This study presents the development and evaluation of a functional prototype of an ultrasonic smart cane designed to enhance user safety and autonomy. Using ultrasonic technology, the cane detects obstacles at distances of up to 100 cm, emitting real-time auditory alerts. Evaluated through laboratory and field tests with real users, the prototype demonstrated high precision in obstacle detection and quick responses, facilitating navigation in complex environments. Key improvements were identified, such as moisture protection and size adjustments for different users. Future developments could include integrating Bluetooth connectivity to expand the device’s functionalities. The results suggest that the ultrasonic smart cane can be a valuable tool for improving the quality of life of visually impaired individuals, providing an accessible and practical solution for independent mobility. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2026.},
	language = {English},
	booktitle = {Lect. {Notes} {Networks} {Syst}.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Loor Guaycha, Andrés Manuel and Matute Palma, José Andrés and Nuñez Panta, Patricia},
	year = {2026},
	note = {Journal Abbreviation: Lect. Notes Networks Syst.},
	keywords = {Assistive Technology, Smart Cane, Ultrasonic Sensor, Visual Impairment},
	pages = {18--30},
	file = {Snapshot:/Users/florian/Zotero/storage/W6NFX3BJ/105019311363.html:text/html},
}

@article{xu_multimodal_2025,
	title = {Multimodal {Navigation} and {Virtual} {Companion} {System}: {A} {Wearable} {Device} {Assisting} {Blind} {People} in {Independent} {Travel}},
	volume = {25},
	issn = {1424-8220},
	shorttitle = {Multimodal {Navigation} and {Virtual} {Companion} {System}},
	doi = {10.3390/s25134223},
	abstract = {Visual impairment or even loss seriously affects quality of life. Benefited by the rapid development of sound/laser detection, Global Positioning System (GPS)/Beidou positioning, machine vision and other technologies, the quality of life of blind people is expected to be improved through visual substitution technology. The existing visual substitution devices still have limitations in terms of safety, robustness, and ease of operation. The remote companion system developed here fully utilizes multimodal navigation and remote communication technologies, and the positioning and interaction functions of commercial mobile phones. Together with the accumulated judgment of backend personnel, it can provide real-time, safe, and reliable navigation services for blind people, helping them complete daily activities such as independent travel, circulation, and shopping. The practical results show that the system not only has strong operability and is easy to use, but also can provide users with a strong sense of security and companionship, making it suitable for promotion. In the future, this system can also be promoted for other vulnerable groups such as the elderly. © 2025 by the authors.},
	language = {English},
	number = {13},
	journal = {Sensors},
	author = {Xu, Jingjing and Wang, Caiyi and Li, Yancheng and Huang, Xuantuo and Zhao, Meina and Shen, Zhuoqun and Liu, Yiding and Wan, Yuxin and Sun, Fengrong and Zhang, Jianhua and Xu, Shengyong},
	year = {2025},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)},
	keywords = {independent travel, navigation, obstacle voidance, remote companion system, wearable systems},
	file = {Snapshot:/Users/florian/Zotero/storage/JGPYUVXH/105010344992.html:text/html},
}
