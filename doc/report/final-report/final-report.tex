\documentclass[sigconf]{acmart}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%%
%% Rights management information. 

\copyrightyear{2025}
\acmYear{2025}
\setcopyright{rightsretained}
\acmConference[AIM 2025]{Proceedings of the UNamur
Symposium on Advanced Interaction Methods}{December 25,
2025}{Namur, Belgium}
\acmBooktitle{Proceedings of the UNamur
Symposium on Advanced Interaction Methods (AIM 2025 Proceedings), December 25, 2025,
Namur, Belgium}\acmDOI{00.0000/0000000.0000000}
\acmISBN{000-0-0000-0000-0/00/00}

%%
%% Packages

\usepackage{xspace}
\usepackage{xcolor}
\usepackage{url}

\newcommand{\ie}{\textit{i.e.},\xspace}
\newcommand{\eg}[0]{\textit{e.g.},\xspace}
\newcommand{\vs}[0]{\textit{vs.}\xspace}
\newcommand{\etal}{\textit{et al.}\xspace}

% Define a command for code formatting
\def\code{\begingroup\urlstyle{tt}\Url}

\begin{document}

%%
%% Title

\title[Adaptive Cane]{Adaptive Cane for Visually Impaired People}

%%
%% Authors

\author{Simon Dourte}
\email{simon.dourte@student.unamur.be}

\author{Edwyn Eben}
\email{edwyn.eben@student.unamur.be}

\author{Nathan Lambrechts}
\email{nathan.lambrechts@student.unamur.be}

\author{Louca Mathieu}
\email{louca.mathieu@student.unamur.be}

\author{Florian Stormacq}
\email{florian.stormacq@student.unamur.be}

\affiliation{
  \institution{University of Namur, Namur Digital Institute (NaDI) and Research Center on Information Systems Engineering (PReCISE)}
  \city{Namur}
  \country{Belgium}
}

\renewcommand{\shortauthors}{Lambrechts et al.}

%%
%% Teaser figure

\begin{teaserfigure}
    \centering
    \vspace{-10pt}
    \includegraphics[width=0.5\textwidth,alt={Adaptive cane system overview}]{../../figures/adaptive-cane-system-final.jpeg}
    \vspace{-15pt}
    \caption{Overview of the Adaptive Cane System with Intel RealSense camera, microphone, Raspberry Pi processing unit, and haptic feedback motors}
    \label{fig:teaser}
\end{teaserfigure}

%%
%% Abstract

\begin{abstract}
People with visual impairments represent a significant proportion of the global population. It is estimated that at least 2.2 billion people have some form of vision impairment. It is therefore essential to develop solutions that meet their needs and improve their independence. With this in mind, this project presents an adaptive connected cane for blind or visually impaired people designed to improve their perception of their surroundings and increase their safety in the face of potential obstacles and dangers. The cane is based on two main sensors: an Intel RealSense D435i depth camera and a USB microphone. The data from these sensors is processed in real time by a Raspberry Pi 3. The microphone assesses the noise level of the environment, while the camera detects the position and distance of obstacles and the recommended area to avoid them. This information is used to determine one of the device's three alert modes. User feedback is provided by three vibrating motors, each associated with a direction. The intensity and duration of the vibrations indicate the proximity of the danger and guide the user as they move. This prototype is therefore an assistance solution aimed at improving the mobility and safety of blind or visually impaired people in their daily lives.
\end{abstract}

%%
%% Keywords and Concepts

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010583.10010588.10010559</concept_id>
       <concept_desc>Hardware~Sensors and actuators</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010583.10010588.10010598.10011752</concept_id>
       <concept_desc>Hardware~Haptic devices</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010583.10010588.10010596</concept_id>
       <concept_desc>Hardware~Sensor devices and platforms</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010520.10010553.10010562.10010563</concept_id>
       <concept_desc>Computer systems organization~Embedded hardware</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10011738.10011775</concept_id>
       <concept_desc>Human-centered computing~Accessibility technologies</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010583.10010588.10003247</concept_id>
       <concept_desc>Hardware~Signal processing systems</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Hardware~Sensors and actuators}
\ccsdesc[500]{Hardware~Haptic devices}
\ccsdesc[300]{Hardware~Sensor devices and platforms}
\ccsdesc[300]{Computer systems organization~Embedded hardware}
\ccsdesc[500]{Human-centered computing~Accessibility technologies}
\ccsdesc[300]{Hardware~Signal processing systems}

\maketitle

%%
%% Introduction

\section{Introduction}

The goal of the project is to develop an adaptive smart cane designed to improve spatial awareness and safety for visually impaired people. The system relies on two main sensors: a depth camera and a microphone to detect obstacles or danger in real time. The data is collected by a Raspberry Pi which analyzes visual and audio information from the environment.

The system focuses entirely on haptic feedback. A set of 3 vibration motors is placed on the cane to provide tactile information to the user. Each motor corresponds to a direction, which means one direction is missing. Indeed, we have left out the back direction because the camera will be positioned as glasses, so it cannot see behind the user. The vibration intensity and duration reflect the proximity of the danger, allowing the user to interpret the environment through the haptic output.

This cane aims to support blind people during urban navigation or any activity that presents potential danger to the user.

%%
%% Related work

\section{Related work}

Several assistive technologies have been developed to enhance mobility for visually impaired individuals. Manuel et al.~\cite{manuel_ai-enhanced_2026} present AI-enhanced smart glasses that combine YOLOv8-based obstacle detection with OCR and image captioning capabilities for comprehensive environmental awareness.

In the domain of smart canes, Loor Guaycha et al.~\cite{loor_guaycha_functional_2026} developed an ultrasonic-based prototype capable of detecting obstacles up to 100 cm away, providing real-time auditory alerts to users. Their field tests demonstrated high precision in obstacle detection.

For multimodal approaches, Xu et al.~\cite{xu_multimodal_2025} propose a wearable navigation system that combines GPS positioning, machine vision, and remote companion functionality, enabling independent travel and daily activities for blind users.

Unlike these approaches, the system developed in this project integrates a depth camera and an audio sensor to provide real-time haptic feedback for obstacle avoidance. This multimodal approach aims to enhance user safety and spatial awareness through tactile signals.

%%
%% Methodology

\section{Methodology}

The project is structured into several key components to ensure optimal functionality, performance, and maintainability.

\subsection{Project Architecture}

\subsubsection{Audio Sensor (Microphone)}

To capture ambient sound, we use a USB microphone (\textit{``USB PnP Sound Device''}). The goal is to continuously record the surrounding audio signal in order to determine the noise level around the user. This allows us to know whether the environment is noisy (\eg a crowd) or quiet.

\subsubsection{Video Sensor (Intel RealSense Camera)}

To capture the image and depth information, we use an Intel RealSense Depth D435i camera connected to the Raspberry Pi via a USB-C cable. The aim is to continuously record the depth stream perceived by the camera in order to locate potential dangers, estimate their distance, and determine the alert mode of the smart cane as well as the direction to avoid obstacles. 

The camera is configured with a resolution of 640×480 pixels and a frame rate of 15 frames per second. Only the depth stream is used in our application.

Each depth image is then divided into three vertical zones (left, center, and right). For each zone, the median distance is calculated to reduce the influence of noise and obtain a more stable estimate of the distance to obstacles.

Two distance thresholds are defined to assess the level of danger: an attention zone when the obstacle is located less than 2 meters away, and an alert zone when it is less than 1 meter away. Any obstacle located beyond 2 meters is considered non-hazardous.

\subsubsection{Processing Unit}

The processing unit used is a Raspberry Pi 3, which handles data acquisition, processing, and communication with the Arduino board. The Raspberry Pi runs multiple threads to manage audio and video data streams, synchronization, and command generation for the output layer.

The producer-consumer architecture is implemented with separate queues for the audio, video, synchronization buffer, and Arduino commands. Each queue has a maximum size to prevent memory overflow: the microphone and video queues can hold up to 10 items each, while the Arduino command queue is limited to 5 items. The queues use a LIFO (Last-In-First-Out) strategy to ensure real-time performance by prioritizing the most recent data. To avoid blocking the processing and communication threads, when a queue reaches capacity, the three oldest items are automatically dropped, and the dropped count is tracked for monitoring purposes.

This architecture allows for efficient and parallel processing of multimodal data, ensuring real-time feedback to the user. The system monitors queue statistics including total items processed, current queue sizes, and processing frequencies, which are logged every 10 seconds to track system performance.

\subsubsection{Arduino Board}

In this project, Arduino is used as an interface between the central processing unit (Raspberry Pi) and the physical actuators (vibration motors). It receives already-processed instructions that specify how the motors should behave.

First, the Raspberry Pi sends data divided into three components: where the danger comes from, the intensity, and the duration of the vibration. The Arduino receives the information, interprets it, and converts it into appropriate signals for the vibration motors.

In the overall architecture, the Arduino is responsible for executing the physical output (vibration motors) in real time. It does not process data; it applies the received instructions to the vibration motors.

\subsection{Project Implementation}

First, to ensure an easy setup of the project, UV was used to manage the Python dependencies. All required libraries are listed in the \code{pyproject.toml} file, allowing for a straightforward installation process.

\subsubsection{Code Structure}

The codebase is organized into several modules, each responsible for a specific functionality of the system. The entry point of the project is the \code{main.py} script, which orchestrates the different threads. This script initializes and starts all the necessary threads for data acquisition, processing, communication, and monitoring of the sent data. The main script supports several command-line arguments: \code{--no-audio} to disable audio capture, \code{--no-video} to disable video capture, \code{--debug} to enable verbose logging, and \code{--simulate} to simulate inputs for testing purposes. Each producer (audio and video) runs as a daemon thread to ensure proper shutdown when the main process terminates.

In addition to this main script, a secondary Python script (\code{monitor_serial.py}) is provided to monitor the serial communication between the Raspberry Pi and the Arduino board. This lightweight monitoring script connects to the Arduino via \code{/dev/ttyACM0} at 115200 baud and displays all incoming messages in real time. To facilitate the startup of the entire system along with the monitoring script, a shell script (\code{start.sh}) is provided that launches both the main system and the monitor simultaneously. The script captures the process ID of the main system and ensures proper cleanup when the monitor is stopped with Ctrl+C.

\subsubsection{Queue Manager}

To simplify the management of the queues used for inter-thread communication, a dedicated module was implemented: \code{queue_manager.py}. This module creates a singleton class, \code{QueueManager}, which encapsulates the functionality of all queues used in the project. The \code{QueueManager} class initializes five separate queues: \code{micro_queue} and \code{video_queue} (maxsize=10) for raw sensor data, \code{audio_processed_queue} and \code{video_processed_queue} (maxsize=5) for processed features, and \code{arduino_queue} (maxsize=5) for commands to be sent to the Arduino.

Each queue is configured with a maximum size to prevent memory overflow. The module provides dedicated methods for adding and retrieving data from each queue (\code{put_micro_data()}, \code{get_micro_data()}, \code{put_video_data()}, etc.). When adding data, the current timestamp is automatically attached as a tuple \code{(data, timestamp)} to facilitate temporal synchronization. The queue manager implements comprehensive monitoring, tracking both the total number of items processed and the number of dropped items for each queue. When a queue becomes full, the system drops the three oldest items before adding the new data, preventing blocking while maintaining real-time responsiveness. The \code{get_queue_stats()} method provides detailed statistics about queue usage, enabling performance monitoring and system diagnostics. Encapsulating the queue management logic within a dedicated module simplifies the code, making it easier to understand and maintain.

\subsubsection{Sensors Implementation}

\subsubsection{Video Sensor (RealSense D435i)}

The \code{camera.py} code uses the RealSense Depth D435i camera to detect obstacles in front of the cane and estimate the distances to them in real time.

It uses the pyrealsense2 library to communicate with the camera. The camera is configured with a resolution of 640×480 pixels at 15 FPS to capture depth images. At startup, a detection phase checks if the camera is available. If it is not detected, the system switches to simulation mode, generating artificial distances to test the code.

The \code{process_frame()} function divides each image into three vertical zones corresponding to the left, center, and right directions. For each zone, the distance is estimated from the depth data measured by the camera. After converting this perceived data to meters, a median is applied to all the values to obtain a representative value for each image using the \code{median_calculator()} function. This approach helps limit measurement errors and sensor noise. To make the measurements even more stable, the system takes into account distances calculated over several successive images for each zone, instead of relying on a single image; this is called smoothing.

The distances from this history are stored, and then a second median is calculated to obtain a more stable final distance. Based on the smoothed central distance, the \code{danger_zone()} function determines which danger zone the user is in (peaceful, caution, or alert). Beyond 2 meters, the user is in peaceful mode, indicating that there are no obstacles that could pose a real danger. Between 2 and 1 meter, the user enters a caution zone, where they know there is an obstacle in front of them and that they must avoid it. Within 1 meter, the user knows they must move if they don't want to collide with the obstacle.

Finally, the \code{start_video_capture()} function manages the main operation of the video module. It has a loop that runs continuously to capture images, but also to calculate distances and detect nearby obstacles in each area. Based on this information, a direction is chosen to avoid the obstacle, and this direction is determined by selecting the area where the distance is greatest (where there are no obstacles). The essential data is then sent to the video data queue via the \code{queue_manager}.

\subsubsection{Audio Sensor (Microphone)}

A Python script in the \code{micro} folder is used to capture the data recorded by the microphone. Audio capture relies on the \code{sounddevice} library, which creates an audio stream that captures samples in real time. The signal is retrieved as chunks, then sent into a queue to be processed. 

To achieve this, the script first searches for the desired audio device. Then, it defines various parameters (chunk size, chunk duration, etc.). After that, audio capture is performed through an audio stream (\code{InputStream}) associated with a callback function. Each time new data is received, the samples are added to an internal buffer. As soon as this buffer contains enough samples to form a complete chunk, the chunk is extracted and converted into a NumPy array before being sent into a queue for processing. Additionally, the module provides a simulation mode to facilitate testing and debugging of the system without requiring physical audio hardware.

\subsubsection{Raspberry Pi Module}

As the Raspberry Pi is the core processing unit of the system, it hosts several modules that handle different aspects of data processing and communication.

The \code{raspberry.py} module is the main module that integrates all functionalities, managing data flow between the audio and video modules, synchronization buffer, and Arduino communication.

As discussed earlier, the project architecture is based on a producer-consumer model, with multiple threads handling different data streams. Each sensor modality has its own dedicated thread for data acquisition and preprocessing, which then feeds the shared queues.

The \code{raspberry.py} script is responsible for all the heavy processing tasks. For audio data, the \code{heavy_audio_processing()} function computes the Root Mean Square (RMS) of the signal, converts it to dB level using the formula $20 \times \log_{10}(\text{rms} / \text{reference})$, and classifies the sound into four categories: ``Chillax'' ($< -45$ dB), ``Some noise'' ($-45$ to $-30$ dB), ``Be Careful'' ($-30$ to $-15$ dB), or ``Danger'' ($> -15$ dB). Additionally, it performs Fast Fourier Transform (FFT) analysis to detect the dominant frequency in the audio signal. For video data, the \code{heavy_video_processing()} function analyzes obstacle positions and distances, computing a danger level (0--3) based on the number and position of detected obstacles, with special priority given to center obstacles. The function also assigns risk classifications (``safe'', ``medium'', ``high'', or ``critical'') corresponding to each danger level.

The script runs separate processing threads for audio and video data (\code{micro_processing_thread()} and \code{video_processing_thread()}), each continuously consuming data from their respective queues. These processed results are then added to intermediate queues before being synchronized using the \code{sync_buffer.py} module. Finally, commands are generated for the Arduino board based on the synchronized information, and these commands are sent via serial communication at 115200 baud.

\paragraph{Synchronization Buffer} 

In order to correctly merge the information from the audio and video sensors, a time synchronization mechanism has been implemented using the \code{sync_buffer.py} module. This module is responsible for associating the processed audio and video data that correspond to the same point in time, despite different capture frequencies and processing times.

The system is based on two separate buffers, implemented as limited-size \code{deque} structures, one for audio data and the other for video data. Each buffer can hold up to five items. The stored data is encapsulated in \code{SensorData} objects, which contain three essential pieces of information: a precise timestamp, the processed sensor data, and the origin of the data (audio or video).

When a new audio or video result is produced by the respective processing modules, it is added to the corresponding buffer with a timestamp obtained using the \code{time.time()} function.

In order to avoid the accumulation of obsolete data and maintain low latency, an automatic cleaning mechanism is applied. Any data older than a maximum threshold of 150 milliseconds is deleted from the buffer.

The synchronization itself is performed by the \code{get_synchronized_pair()} method. This method compares all possible combinations between the elements present in the audio and video buffers in order to determine the pair with the smallest time difference. If this difference is less than a synchronization threshold set at 50 milliseconds, the pair is considered valid. The elements used are then removed from the buffers to prevent any subsequent reuse. The value of 50 milliseconds was chosen arbitrarily based on what worked best for our tests. 

If no sufficiently synchronized pair is found, the method returns a null value, indicating that no reliable multimodal fusion can be performed at that time. The module also provides methods for retrieving the most recent audio or video data, which are used as a fallback mechanism when one of the two modalities is temporarily unavailable.

\paragraph{Intensity Calculator} 

To adjust the output intensities based on the processed sensor data, the \code{intensity_calculator.py} script is used. This module implements the \code{IntensityCalculator} class with two static methods for converting sensor data into intensity values ranging from 0 to 100. These values represent the perceived level of risk and are used to modulate the intensity of the feedback provided to the user. The class is based on two distinct static methods, corresponding to the two sensory modalities of the system: audio and vision.

\textbf{Audio conversion:}
The \code{audio_to_intensity()} method converts the ambient sound level, expressed in decibels (dB), into an intensity between 0 and 100. This conversion is performed using a piecewise linear function, defined over several sound level intervals in order to gradually reflect the increase in risk associated with ambient noise. Four sound level ranges are defined:

\begin{itemize}
    \item For levels below $-45$ dB, corresponding to a very quiet environment, the intensity varies linearly from 0 to 20.
    \item Between $-45$ dB and $-30$ dB, the environment is considered moderately noisy, and the intensity increases gradually from 20 to 50.
    \item Between $-30$ dB and $-15$ dB, the noise becomes significant and the intensity is between 50 and 80.
    \item Above $-15$ dB, the sound level is considered potentially dangerous and the intensity is saturated between 80 and 100 in order to avoid excessive values.
\end{itemize}

\textbf{Visual conversion by zone:}
The \code{vision_to_intensity_by_zone()} method converts the distances estimated by the depth camera into distinct intensities for the three spatial zones considered: left, center, and right. For each zone, a default distance of 5 meters is used in the absence of a valid measurement. The base intensity is calculated based on the minimum distance detected in the zone:

\begin{itemize}
    \item For distances greater than 2 meters, the intensity remains low (0 to 15), indicating a non-hazardous situation.
    \item For distances between 1 and 2 meters, the intensity increases linearly from 15 to 70, indicating an area requiring vigilance.
    \item For distances less than 1 meter, the intensity is high (70 to 100), signaling immediate danger.
\end{itemize}

In addition to this distance-based estimate, a reinforcement mechanism is applied when an obstacle is explicitly detected in a given area. In this case, a fixed bonus of 20 points is added to the calculated intensity. This increase allows obstacles clearly identified by the vision algorithm to be prioritized, even if their distance remains slightly above the critical thresholds. Finally, all intensities are limited between 0 and 100 to ensure system stability and compatibility with the output device control protocol.

\paragraph{Message Generator} 

A final relevant module is the \code{lcr_message_generator.py}, which is responsible for formatting the computed intensity levels into the specific command protocol required by the Arduino board. This module implements the \code{LCRMessageGenerator} class that maintains state including the last sent message and a message counter for tracking.

The command format used is \code{LxxxCxxxRxxx}, where \code{L}, \code{C}, and \code{R} represent the left, center, and right output intensities, respectively, each followed by a three-digit zero-padded intensity value (000--100). The module provides two generation methods: \code{generate_synchronized_message()} for cases where both audio and video data are available, and \code{generate_fallback_message()} for single-modality scenarios.

The synchronization method implements a sophisticated weighted averaging scheme to merge audio and video intensities. For the center zone, the formula is: $(4 \times \text{vision\_intensity} + \text{audio\_intensity}) / 5$, giving 80\% weight to visual data and 20\% to audio. For lateral zones (left and right), audio influence is further reduced to 70\% of its original value: $(4 \times \text{vision\_intensity} + 0.7 \times \text{audio\_intensity}) / 5$. This weighting strategy prioritizes visual obstacle detection while still incorporating ambient sound information, particularly emphasizing visual data for the critical center zone where the user is heading. The weighted average calculation ensures smooth transitions between different environmental conditions.

These weighting values can be modified in the \code{lcr_message_generator.py} module to adjust the system's sensitivity according to user preferences, and this opens the possibility for future improvements, such as implementing a user-tunable sensitivity setting.

All these modules work together seamlessly to ensure that the system operates in real time, providing appropriate feedback to the user based on the multimodal sensory input. The entire processing pipeline---from sensor data acquisition through queue management, processing, synchronization, intensity calculation, and message generation---is designed to maintain low latency and high responsiveness, crucial for assistive navigation applications.

\subsubsection{Arduino Board Implementation}

The Arduino is used as a hardware interface between the Raspberry Pi and the output devices. Its role is limited to receiving commands sent by the Raspberry Pi in real time and applying them to the vibration motors.

The Arduino communicates with the Raspberry Pi via a serial connection configured at 115200 baud. Commands are transmitted using a simple and robust format: \code{LxxxCyyyRzzz}, where each value represents the intensity of the danger expressed between 0 and 100. The letters stand for left, center, and right directions.

Upon reception, each command is directly delivered to the Arduino side. The received values in the range of 0--100 are converted to an 8-bit scale (0--255) required for proper interpretation by the Arduino. In this part, there is no buffer or queue; each new command immediately replaces the previous output state. This choice ensures that the feedback always reflects the most recent perception of the environment.

Originally, the system was designed to accommodate three vibration motors, one for each spatial direction. Due to hardware and voltage constraints, these motors were first replaced by simple LEDs. However, the brightness of the single color of the LEDs was not fully reliable because the intensity values were difficult to distinguish visually, making it hard to verify whether the Arduino was correctly interpreting the intensity of the danger. To overcome this limitation, RGB LEDs are used as a visual proxy for the haptic feedback. Each LED corresponds to one direction, and its color and brightness simulate the vibration intensity. The color can smoothly transition from green (safe) through yellow and orange (increasing risk) to red (critical danger), providing a clearer and more intuitive representation of the intensity level.

The Arduino code is structured into multiple C++ files:
\begin{itemize}
    \item A main file (\code{main.cpp}) is used to handle initialization and the main execution loop.
    \item A get file (\code{get.cpp}) that manages the serial input.
    \item The logic file (\code{logic.cpp}) is used to parse and convert the intensity values. The RGB architecture is also present here.
    \item The send file (\code{send.cpp}) is used to abstract the hardware control of the output devices.
\end{itemize}

In addition, three corresponding header files (\code{get.h}, \code{logic.h}, and \code{send.h}) are used so that the functions of these files can be used across the different files of the project.

Several hardware considerations had to be addressed. Indeed, directly powering vibration motors from the Arduino was unsafe due to current limitations and voltage mismatches, which could damage the Arduino board. Using LEDs as a placeholder allowed functional testing of the communication from the Raspberry Pi to the outputs and control logic while avoiding electrical risks and hardware damage. Integrating proper motor drivers remains future work, as well as environmental tests.

%%
%% Contribution

\section{Contribution}

This project was developed by our student group; each team member was responsible for specific components of the system to ensure a clear separation of tasks and efficient development.

Edwyn Eben was responsible for the video detection module with the RealSense Depth D435i camera, obtaining depth data, obstacle detection, and distance-based risk assessment.

Louca Mathieu was responsible for capturing audio data (\code{micro.py}) and synchronizing audio and visual data. 

Nathan Lambrechts developed the Arduino board code, handling serial communication and output device control. The hardware integration and safety considerations were also managed by him.

Florian Stormacq implemented the Raspberry Pi processing unit, including configuration and dependencies management, multi-threading architecture, the initial queue manager, and the main processing pipeline.

%%
%% Evaluation

\section{Evaluation}

The system was not evaluated in real-world conditions due to time constraints and the unavailability of certain hardware components. However, each prototype module was tested individually to ensure correct functionality. Additionally, the prototype was tested in a controlled environment to validate the overall system integration, as well as the visual feedback provided by the LED strip.

%%
%% Discussion

\section{Discussion}

\subsection{Limitations}

To begin with, the choice of hardware components imposed certain limitations on the system's performance. To ensure real-time processing, the Raspberry Pi 3 was selected as the processing unit. To replicate this project, it is important to use a Raspberry Pi 3 or a more powerful model, as older versions struggle to handle the computational load of processing all sensor data in real time.

Another limitation of the prototype is the limited processing rate of the audio module. While the video module can process data at a rate of approximately 15 frames per second, the audio module is limited to processing only 6 items per second. This leads to a situation where audio data becomes the bottleneck for the overall system performance, as the synchronization buffer assembles data based on the produced timestamps. Consequently, the system can only provide updates at a maximum rate of 6 items per second, which may not be sufficient for fast-changing environments.

A final limitation to consider is the absence of real-world testing with visually impaired users. Due to time constraints and the unavailability of certain hardware components, the system could not be evaluated in practical scenarios. Real-world testing is crucial for assessing the effectiveness and usability of the system, as well as for gathering feedback from actual users to inform future improvements.

\subsection{Problems Encountered}

During the development of this project, several challenges were encountered that required problem-solving and adaptation. Some of the most notable issues included:

\begin{enumerate}
\item Configuring the Raspberry Pi to work with the RealSense camera, which involved building the PyRealSense2 library from source. This process was time-consuming and error-prone, requiring careful attention to detail and extensive troubleshooting. For more information, please refer to the documentation provided in the GitHub repository.
\item The initial project aimed to use vibration motors as output devices, one for each direction (left, center, right). These motors would have been placed on a handlebar to provide haptic feedback to the user. However, due to time limitations and difficulties in assembling the desired prototype, the vibration motors were replaced by an LED strip for visual feedback. This change allowed for a simpler and quicker implementation while still demonstrating the core functionality of the system.
\item Limited documentation for the PyRealSense2 library made it difficult to install and utilize the camera effectively across different operating systems.
\item Debugging multi-threaded applications proved to be, as expected, a complex task. Ensuring that all threads operated correctly and efficiently required careful design, testing, and the implementation of comprehensive logging and monitoring systems.
\item Synchronizing audio and video data streams accurately to ensure that the output feedback corresponded correctly to the sensory input, particularly given the different capture rates and processing times of each modality.
\item Saturating the Arduino communication buffer when sending commands too rapidly, which led to unresponsive output devices. To address this issue, a monitoring script (\code{monitor_serial.py}) was implemented to visualize the commands being sent to the Arduino in real time, allowing for better debugging and optimization of the communication process.
\end{enumerate}

\subsection{Ideas for Future Work}

As this project was developed within a limited timeframe, there are several areas of improvement and expansion that could be explored in future work. Some potential directions for future development include:

\begin{enumerate}
\item Enhancing user interaction by implementing a user interface that allows for real-time adjustment of system parameters, such as the weighted averaging coefficients between audio and video inputs, sensitivity thresholds, and output modes.
\item Integrating the originally intended vibration motors to provide haptic feedback and evaluating their effectiveness compared to the visual feedback provided by the LED strip. This modification would not require significant changes to the existing codebase, as the communication protocol with the Arduino would remain the same.
\item Optimizing the audio processing module to increase the data processing rate beyond the current 6 items per second, allowing for more frequent updates and smoother feedback. This could involve implementing more efficient FFT algorithms or utilizing hardware acceleration.
\item Conducting real-world testing with visually impaired users to evaluate the system's performance in various environments and scenarios, providing valuable insights into its practical usability, effectiveness, and user acceptance. Such studies would also help identify necessary adjustments to the sensitivity settings and feedback mechanisms.
\item Implementing adaptive sensitivity that automatically adjusts based on environmental conditions and user preferences learned over time.
\end{enumerate}

%%
%% Conclusion

\section{Conclusion}

This project successfully developed a multimodal assistive system that integrates audio and video sensors and provides real-time feedback to the user through a set of output devices. The system architecture, based on a producer-consumer model with multi-threading and sophisticated queue management, effectively handles the complexities of data acquisition, time-sensitive processing, temporal synchronization, and communication. The implementation includes comprehensive modules for audio and video processing, intensity calculation using weighted averaging, and command generation following a well-defined protocol.

As with all prototype projects, there are several areas for improvement that could be explored in future work, including enhancing user interaction through adaptive interfaces, optimizing processing modules for higher throughput, integrating the intended haptic feedback hardware, and conducting real-world evaluations with target users. Overall, this project demonstrates the feasibility and potential of multimodal systems for enhancing mobility and safety for visually impaired individuals through intelligent sensory feedback.

%%
%% Open Science

\section*{Open Science}

The complete code for this project is available on GitHub at the following repository: \url{https://github.com/fstormacq/Master1-IIA-project}

%%
%% Acknowledgements

\begin{acks}
    This work was supported by the University of Namur, the Namur Digital Institute (NaDI), and the Research Center on Information Systems Engineering (PReCISE). The authors would like to thank all individuals who contributed to this project. A particular thanks to the providers of the hardware components that made this research possible.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{./bibliography.bib}

\end{document}
\endinput