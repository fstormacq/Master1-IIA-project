\documentclass[sigconf]{acmart}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%%
%% Rights management information. 

\copyrightyear{2025}
\acmYear{2025}
\setcopyright{rightsretained}
\acmConference[AIM 2025]{Proceedings of the UNamur
Symposium on Advanced Interaction Methods}{December 25,
2025}{Namur, Belgium}
\acmBooktitle{Proceedings of the UNamur
Symposium on Advanced Interaction Methods (AIM 2025 Proceedings), December 25, 2025,
Namur, Belgium}\acmDOI{00.0000/0000000.0000000}
\acmISBN{000-0-0000-0000-0/00/00}

%%
%% Packages

\usepackage{xspace}
\usepackage{xcolor}
\newcommand{\ie}{\textit{i.e.},\xspace}
\newcommand{\eg}[0]{\textit{e.g.},\xspace}
\newcommand{\vs}[0]{\textit{vs.}\xspace}
\newcommand{\etal}{\textit{et al.}\xspace}

\begin{document}

%%
%% Title

\title[Adaptive Cane]{Adaptive Cane for Visually Impaired People}

%%
%% Authors

\author{Simon Dourte}
\email{simon.dourte@student.unamur.be}

\author{Edwyn Eben}
\email{edwyn.eben@student.unamur.be}

\author{Nathan Lambrechts}
\email{nathan.lambrechts@student.unamur.be}

\author{Louca Mathieu}
\email{louca.mathieu@student.unamur.be}

\author{Florian Stormacq}
\email{florian.stormacq@student.unamur.be}

\affiliation{
  \institution{University of Namur, Namur Digital Institute (NaDI) and Research Center on Information Systems Engineering (PReCISE)}
  \city{Namur}
  \country{Belgium}
}

\renewcommand{\shortauthors}{Lambrechts et al.}

%%
%% Teaser figure

\begin{teaserfigure}
    \centering
    \vspace{-10pt}
    \includegraphics[width=0.5\textwidth,alt={Adaptive cane system overview}]{../figures/adaptive-cane-system-final.jpeg}
    \vspace{-15pt}
    \caption{Overview of the Adaptive Cane System with Intel RealSense camera, microphone, Raspberry Pi processing unit, and haptic feedback motors}
    \label{fig:teaser}
\end{teaserfigure}

%%
%% Abstract

\begin{abstract}
People with visual impairments represent a significant proportion of the global population. It is estimated that at least 2.2 billion people have some form of vision impairment. It is therefore essential to develop solutions that meet their needs and improve their independence. With this in mind, this project presents an adaptive connected cane for blind or visually impaired people designed to improve their perception of their surroundings and increase their safety in the face of potential obstacles and dangers. The cane is based on two main sensors: an Intel RealSense D435 depth camera and a USB microphone. The data from these sensors is processed in real time by a Raspberry Pi 3. The microphone assesses the noise level of the environment, while the camera detects the position and distance of obstacles and the recommended area to avoid them. This information is used to determine one of the device's three alert modes. User feedback is provided by three vibrating motors, each associated with a direction. The intensity and duration of the vibrations indicate the proximity of the danger and guide the user as they move. This prototype is therefore an assistance solution aimed at improving the mobility and safety of blind or visually impaired people in their daily lives.
\end{abstract}

%%
%% Keywords and Concepts

\begin{CCSXML}
\textit{The CCS concepts will be completed later.}
\end{CCSXML}

\maketitle

%%
%% Introduction

\section{Introduction}

The goal of the project is to develop an adaptive smart cane designed to improve spatial awareness and safety for visually impaired people. The system relies on two main sensors: a depth camera and a microphone to detect obstacles or danger in real-time. The data is collected by a Raspberry Pi which analyzes visual and audio information from the environment.

The system focuses entirely on haptic feedback. A set of 3 vibration motors is placed on the cane to provide tactile information to the user. Each motor has a direction which means one direction is missing. Indeed, we have left out the back direction because the camera will be settled as glasses so they cannot see behind the user. The vibration intensity and duration reflect the proximity of the danger, allowing the user to interpret the environment with the haptic output.

This cane aims to support blind people during urban navigation or any activity that has potential danger to the user.

%%
%% Related work

\section{Related work}

% ============================================================================
% TODO: TO BE COMPLETED BY TEAM MEMBERS - URGENT
% ============================================================================
% Please conduct literature review and add:
% - Existing assistive devices for visually impaired people
% - Multimodal navigation systems
% - Haptic feedback research
% - Similar projects using depth cameras and/or audio sensors
% - Key differences between existing work and our approach
% ============================================================================

\textcolor{red}{\textbf{[TODO: Related Work section to be completed - literature review needed]}}

\textit{Note: A preliminary reference exists about haptic feedback for navigation. This needs to be expanded significantly.}

%%
%% Methodology

\section{Methodology}

The project is structured into several key components to ensure optimal functionality, performance, and maintainability.

\subsection{Project Architecture}

\subsubsection{Audio Sensor (Microphone)}
 To capture ambient sound, we use a USB microphone (\textit{``USB PnP Sound Device''}). The goal is to continuously record the surrounding audio signal in order to determine the noise level around the user. This allows us to know whether the environment is noisy (e.g., a crowd) or quiet.

\subsubsection{Video Sensor (Intel RealSense Camera)}
To capture the image and depth information, we use an Intel RealSense Depth D435i camera connected to the Raspberry Pi via a USB-C cable. The aim is to continuously record the depth stream perceived by the camera in order to locate potential dangers, estimate their distance, and determine the alert mode of the smart cane as well as the direction to avoid obstacles. 

The camera is configured with a resolution of 640×480 pixels and a frame rate of 15 frames per second. Only the depth stream is used in our application.

Each depth image is then divided into three vertical zones (left, center, and right). For each zone, the median distance is calculated to reduce the influence of noise and obtain a more stable estimate of the distance to obstacles.

Two distance thresholds are defined to assess the level of danger: an attention zone when the obstacle is located less than 2 meters away, and an alert zone when it is less than 1 meter away. Any obstacle located beyond 2 meters is considered non-hazardous.

\subsubsection{Processing Unit}

The processing unit used is a Raspberry Pi 3, which handles data acquisition, processing, and communication with the Arduino board. The Raspberry Pi runs multiple threads to manage audio and video data streams, synchronization, and command generation for the output layer.

The producer-consumer architecture is implemented with separate queues for the audio, video, synchronization buffer, and Arduino commands. Each queue has a maximum size to prevent memory overflow: the microphone and video queues can hold up to 10 items each, while the Arduino command queue is limited to 5 items. The queues use a LIFO (Last-In-First-Out) strategy to ensure real-time performance by prioritizing the most recent data. To avoid blocking the processing and communication threads, when a queue reaches capacity, the three oldest items are automatically dropped, and the dropped count is tracked for monitoring purposes.

This architecture allows for efficient and parallel processing of multimodal data, ensuring real-time feedback to the user. The system monitors queue statistics including total items processed, current queue sizes, and processing frequencies, which are logged every 10 seconds to track system performance.

\textbf{Note:} This device is not the initial Raspberry Pi 1 that was furnished at the beginning of the project, as it was not powerful enough to handle the processing requirements.

\subsubsection{Arduino Board}

In this project, Arduino is used as an interface between the central processing unit (Raspberry Pi) and the physical actuators (vibration motors). It receives already-processed instructions that specify how the motors should behave.

First, the Raspberry Pi sends data divided into three components: where the danger comes from, the intensity, and the duration of the vibration. The Arduino receives the information, interprets it, and converts it into appropriate signals for the vibration motors.

In the overall architecture, the Arduino is responsible for executing the physical output (vibration motors) in real-time. It does not process data; it applies the received instructions for the vibration motors.

\subsection{Project Implementation}

First, to ensure an easy setup of the project, UV was used to manage the Python dependencies. All required libraries are listed in the \texttt{pyproject.toml} file, allowing for a straightforward installation process.

\subsubsection{Code Structure}

The codebase is organized into several modules, each responsible for a specific functionality of the system. The entry point of the project is the \texttt{main.py} script, which orchestrates the different threads. This script initializes and starts all the necessary threads for data acquisition, processing, communication, and monitoring of the sent data. The main script supports several command-line arguments: \texttt{--no-audio} to disable audio capture, \texttt{--no-video} to disable video capture, \texttt{--debug} to enable verbose logging, and \texttt{--simulate} to simulate inputs for testing purposes. Each producer (audio and video) runs as a daemon thread to ensure proper shutdown when the main process terminates.

In addition to this main script, a secondary Python script (\texttt{monitor\_serial.py}) is provided to monitor the serial communication between the Raspberry Pi and the Arduino board. This lightweight monitoring script connects to the Arduino via \texttt{/dev/ttyACM0} at 115200 baud and displays all incoming messages in real-time. To facilitate the startup of the entire system along with the monitoring script, a shell script (\texttt{start.sh}) is provided that launches both the main system and the monitor simultaneously. The script captures the process ID of the main system and ensures proper cleanup when the monitor is stopped with Ctrl+C.

\subsubsection{Queue Manager}

To simplify the management of the queues used for inter-thread communication, a dedicated module was implemented: \texttt{queue\_manager.py}. This module creates a singleton class, \texttt{QueueManager}, which encapsulates the functionality of all queues used in the project. The \texttt{QueueManager} class initializes five separate queues: \texttt{micro\_queue} and \texttt{video\_queue} (maxsize=10) for raw sensor data, \texttt{audio\_processed\_queue} and \texttt{video\_processed\_queue} (maxsize=5) for processed features, and \texttt{arduino\_queue} (maxsize=5) for commands to be sent to the Arduino.

Each queue is configured with a maximum size to prevent memory overflow. The module provides dedicated methods for adding and retrieving data from each queue (\texttt{put\_micro\_data ()}, \texttt{get\_micro\_data ()}, \texttt{put\_video\_data ()}, etc.). When adding data, the current timestamp is automatically attached as a tuple \texttt{(data, timestamp)} to facilitate temporal synchronization. The queue manager implements comprehensive monitoring, tracking both the total number of items processed and the number of dropped items for each queue. When a queue becomes full, the system drops the three oldest items before adding the new data, preventing blocking while maintaining real-time responsiveness. The \texttt{get\_queue\_stats ()} method provides detailed statistics about queue usage, enabling performance monitoring and system diagnostics. Encapsulating the queue management logic within a dedicated module simplifies the code, making it easier to understand and maintain.

\subsubsection{Sensors Implementation}

% ============================================================================
% TODO: TO BE COMPLETED BY TEAM MEMBERS
% ============================================================================
% Please describe:

% - Video Sensor implementation (pyrealsense2, depth processing, obstacle detection)
% - Data capture workflow and real-time processing loop
% ============================================================================

\textcolor{red}{\textbf{[TODO: Audio and Video Sensor implementation sections to be completed by team members]}}
\subsubsection{Video Sensor (Realsense D435i)}

The camera.py code uses the RealSense depth i435 camera to detect obstacles in front of the cane and estimate the distances to them in real time.

It uses the pyrealsense2 library to communicate with the camera. The camera is configured with a resolution of 640×480 pixels at 15 FPS to capture depth images. At startup, a detection phase checks if the camera is available. If it is not detected, the system switches to simulation mode, generating artificial distances to test the code.

The process_frame() function divides each image into three vertical zones corresponding to the left, center, and right directions. For each zone, the distance is estimated from the depth data measured by the camera. After converting this perceived data to meters, a median is applied to all the values ​​to obtain a representative value for each image using the median_calculator() function. This approach helps limit measurement errors and sensor noise. To make the measurements even more stable, the system takes into account distances calculated over several successive images for each zone, instead of relying on a single image; this is called smoothing.

The distances from this history are stored, and then a second median is calculated to obtain a more stable final distance. Based on the smoothed central distance, the Danger_zone() function determines which danger zone the user is in (peaceful, caution, or alert). Beyond 2 meters, the user is in peaceful mode, indicating that there are no obstacles that could pose a real danger. Between 2 and 1 meter, the user enters a caution zone, where they know there is an obstacle in front of them and that they must avoid it. Within 1 meter, the user knows they must move if they don't want to collide with the obstacle.

Finally, the start_video_capture() function manages the main operation of the video module. It has a loop that runs continuously to capture images, but also to help calculate distances and detect nearby obstacles in each area. Based on this information, a direction is chosen to avoid the obstacle, and this direction is determined by selecting the area where the distance is greatest (where there are no obstacles). The essential data is then sent to the video data queue via the queue_manager.

\subsubsection{Audio Sensor (Microphone)}

A Python script in the \texttt{micro} folder is used to capture the data recorded by the microphone. Audio capture relies on the \texttt{sounddevice} library, which creates an audio stream that captures samples in real time. The signal is retrieved as chunks, then sent into a queue to be processed. 

To achieve this, the script first searches for the desired audio device. Then, it defines various parameters (chunk size, chunk duration, etc). After that, audio capture is performed through an audio stream (\texttt{InputStream}) associated with a callback function. Each time new data is received, the samples are added to an internal buffer. As soon as this buffer contains enough samples to form a complete chunk, the chunk is extracted and converted into a NumPy array before being sent into a queue for processing. Additionally, the module provides a simulation mode to facilitate testing and debugging of the system without requiring physical audio hardware.

\subsubsection{Raspberry Pi Module}

As the Raspberry Pi is the core processing unit of the system, it hosts several modules that handle different aspects of data processing and communication.

The \texttt{raspberry.py} module is the main module that integrates all functionalities, managing data flow between the audio and video modules, synchronization buffer, and Arduino communication.

As discussed earlier, the project architecture is based on a producer-consumer model, with multiple threads handling different data streams. Each sensor modality has its own dedicated thread for data acquisition and pre-processing, which then feeds the shared queues.

The \texttt{raspberry.py} script is responsible for all the heavy processing tasks. For audio data, the \texttt{heavy\_audio\_processing()} function computes the Root Mean Square (RMS) of the signal, converts it to dB level using the formula $20 \times \log_{10}(\text{rms} / \text{reference})$, and classifies the sound into four categories: "Chillax" (< -45 dB), "Some noise" (-45 to -30 dB), "Be Careful" (-30 to -15 dB), or "Danger" (> -15 dB). Additionally, it performs Fast Fourier Transform (FFT) analysis to detect the dominant frequency in the audio signal. For video data, the \texttt{heavy\_video\_processing()} function analyzes obstacle positions and distances, computing a danger level (0-3) based on the number and position of detected obstacles, with special priority given to center obstacles. The function also assigns risk classifications ("safe", "medium", "high", or "critical") corresponding to each danger level.

The script runs separate processing threads for audio and video data (\texttt{micro\_processing\_thread()} and \texttt{video\_processing\_thread()}), each continuously consuming data from their respective queues. These processed results are then added to intermediate queues before being synchronized using the \texttt{sync\_buffer.py} module. Finally, commands are generated for the Arduino board based on the synchronized information, and these commands are sent via serial communication at 115200 baud.

\paragraph{Synchronization Buffer:} 

In order to correctly merge the information from the audio and video sensors, a time synchronisation mechanism has been implemented using the \texttt{sync\_buffer.py} module. This module is responsible for associating the processed audio and video data that correspond to the same point in time, despite different capture frequencies and processing times.

The system is based on two separate buffers, implemented as limited-size \texttt{deque} structures, one for audio data and the other for video data. Each buffer can hold up to five items. The stored data is encapsulated in \texttt{SensorData} objects, which contain three essential pieces of information: a precise timestamp, the processed sensor data, and the origin of the data (audio or video).

When a new audio or video result is produced by the respective processing modules, it is added to the corresponding buffer with a timestamp obtained using the \texttt{time.time()} function.

In order to avoid the accumulation of obsolete data and maintain low latency, an automatic cleaning mechanism is applied. Any data older than a maximum threshold of 150 milliseconds is deleted from the buffer.

The synchronisation itself is performed by the \texttt{get\_synchronized\_pair()} method. This method compares all possible combinations between the elements present in the audio and video buffers in order to determine the pair with the smallest time difference. If this difference is less than a synchronisation threshold set at 50 milliseconds, the pair is considered valid. The elements used are then removed from the buffers to prevent any subsequent reuse. The value of 50 milliseconds was chosen arbitrarily based on what worked best for our tests. 

If no sufficiently synchronised pair is found, the method returns a null value, indicating that no reliable multimodal fusion can be performed at that time. The module also provides methods for retrieving the most recent audio or video data, which are used as a fallback mechanism when one of the two modalities is temporarily unavailable.

\paragraph{Intensity Calculator:} 

To adjust the output intensities based on the processed sensor data, the \texttt{intensity\_calculator.py} script is used. This module implements the \texttt{IntensityCalculator} class with two static methods for converting sensor data into intensity values ranging from 0 to 100. These values represent the perceived level of risk and are used to modulate the intensity of the feedback provided to the user. The class is based on two distinct static methods, corresponding to the two sensory modalities of the system: audio and vision.

\textbf{Audio conversion:}
The \texttt{audio\_to\_intensity()} method converts the ambient sound level, expressed in decibels (dB), into an intensity between 0 and 100. This conversion is performed using a piecewise linear function, defined over several sound level intervals in order to gradually reflect the increase in risk associated with ambient noise. Four sound level ranges are defined:

\begin{itemize}
    \item For levels below $-45$ dB, corresponding to a very quiet environment, the intensity varies linearly from 0 to 20.
    \item Between $-45$ dB and $-30$ dB, the environment is considered moderately noisy, and the intensity increases gradually from 20 to 50.
    \item Between $-30$ dB and $-15$ dB, the noise becomes significant and the intensity is between 50 and 80.
    \item Above $-15$ dB, the sound level is considered potentially dangerous and the intensity is saturated between 80 and 100 in order to avoid excessive values.
\end{itemize}

\textbf{Visual conversion by zone:}
The \texttt{vision\_to\_intensity\_by\_zone()} method converts the distances estimated by the depth camera into distinct intensities for the three spatial zones considered: left, centre and right. For each zone, a default distance of 5 metres is used in the absence of a valid measurement. The base intensity is calculated based on the minimum distance detected in the zone:

\begin{itemize}
    \item For distances greater than 2 metres, the intensity remains low (0 to 15), indicating a non-hazardous situation.
    \item For distances between 1 and 2 metres, the intensity increases linearly from 15 to 70, indicating an area requiring vigilance.
    \item For distances less than 1 metre, the intensity is high (70 to 100), signalling immediate danger.
\end{itemize}

In addition to this distance-based estimate, a reinforcement mechanism is applied when an obstacle is explicitly detected in a given area. In this case, a fixed bonus of 20 points is added to the calculated intensity. This increase allows obstacles clearly identified by the vision algorithm to be prioritised, even if their distance remains slightly above the critical thresholds. Finally, all intensities are limited between 0 and 100 to ensure system stability and compatibility with the output device control protocol.

\paragraph{Message Generator:} 

A final relevant module is the \texttt{lcr\_message\_generator.py}, which is responsible for formatting the computed intensity levels into the specific command protocol required by the Arduino board. This module implements the \texttt{LCRMessageGenerator} class that maintains state including the last sent message and a message counter for tracking.

The command format used is \texttt{LxxxCxxxRxxx}, where \texttt{L}, \texttt{C}, and \texttt{R} represent the left, center, and right output intensities, respectively, each followed by a three-digit zero-padded intensity value (000-100). The module provides two generation methods: \texttt{generate\_synchronized\_message()} for cases where both audio and video data are available, and \texttt{generate\_fallback\_message()} for single-modality scenarios.

The synchronization method implements a sophisticated weighted averaging scheme to merge audio and video intensities. For the center zone, the formula is: $(4 \times \text{vision\_intensity} + \text{audio\_intensity}) / 5$, giving 80\% weight to visual data and 20\% to audio. For lateral zones (left and right), audio influence is further reduced to 70\% of its original value: $(4 \times \text{vision\_intensity} + 0.7 \times \text{audio\_intensity}) / 5$. This weighting strategy prioritizes visual obstacle detection while still incorporating ambient sound information, particularly emphasizing visual data for the critical center zone where the user is heading. The weighted average calculation ensures smooth transitions between different environmental conditions.

These weighting values can be modified in the \texttt{lcr\_message\_generator.py} module to adjust the system's sensitivity according to user preferences, and this opens the possibility for future improvements, such as implementing a user-tunable sensitivity setting.

All these modules work together seamlessly to ensure that the system operates in real-time, providing appropriate feedback to the user based on the multimodal sensory input. The entire processing pipeline---from sensor data acquisition through queue management, processing, synchronization, intensity calculation, and message generation---is designed to maintain low latency and high responsiveness, crucial for assistive navigation applications.

\subsubsection{Arduino Board Implementation}

% ============================================================================
% TODO: TO BE COMPLETED BY TEAM MEMBERS
% ============================================================================
% Please describe:
% - Arduino board specifications and setup
% - Serial communication protocol details (115200 baud, LxxxCxxxRxxx format)
% - Command parsing and execution on Arduino side
% - Output device control (LED strips as placeholder for vibration motors)
% - Code organization (get.cpp, logic.cpp, send.cpp, main.cpp)
% - Hardware considerations and voltage issues
% ============================================================================

\textcolor{red}{\textbf{[TODO: Arduino implementation section to be completed by team members]}}

\textit{Note: The Arduino module receives pre-processed commands from the Raspberry Pi via serial communication at 115200 baud using the LxxxCxxxRxxx protocol format.}

%%
%% Contribution

\section{Contribution}

% ============================================================================
% TODO: TO BE COMPLETED BY TEAM MEMBERS
% ============================================================================

\textcolor{red}{\textbf{[TODO: Contribution section to be completed by team members]}}

This project was developed by our student group; each team member was responsible for specific components of the system to ensure a clear separation of tasks and efficient development.

Edwyn Eben was responsible for the video detection module with the RealSense Depth i435 camera, obtaining depth data, obstacle detection and distance-based risk assessment.

Louca Mathieu was responsible for capturing audio data (micro.py) and synchronising audio and visual data. 

\textit{Preliminary notes: The project introduces a multimodal approach combining depth and ambient audio information. Directional haptic feedback conveys both presence and position of obstacles.}

%%
%% Evaluation

\section{Evaluation}

The system was not evaluated in real-world conditions due to time constraints and the unavailability of certain hardware components. However, each prototype module was tested individually to ensure correct functionality. Additionally, the prototype was tested in a controlled environment to validate the overall system integration, as well as the visual feedback provided by the LED strip.

%%
%% Discussion

\section{Discussion}

\subsection{Limitations}

To begin with, the choice of hardware components imposed certain limitations on the system's performance. To ensure real-time processing, the Raspberry Pi 3 was selected as the processing unit. To replicate this project, it is important to use a Raspberry Pi 3 or a more powerful model, as older versions struggle to handle the computational load of processing all sensor data in real-time.

Another limitation of the prototype is the limited processing rate of the audio module. While the video module can process data at a rate of approximately 15 frames per second, the audio module is limited to processing only 6 items per second. This leads to a situation where audio data becomes the bottleneck for the overall system performance, as the synchronization buffer assembles data based on the produced timestamps. Consequently, the system can only provide updates at a maximum rate of 6 items per second, which may not be sufficient for fast-changing environments.

A final limitation to consider is the absence of real-world testing with visually impaired users. Due to time constraints and the unavailability of certain hardware components, the system could not be evaluated in practical scenarios. Real-world testing is crucial for assessing the effectiveness and usability of the system, as well as for gathering feedback from actual users to inform future improvements.

\subsection{Problems Encountered}

During the development of this project, several challenges were encountered that required problem-solving and adaptation. Some of the most notable issues included:

\begin{enumerate}
\item Configuring the Raspberry Pi to work with the RealSense camera, which involved building the PyRealSense2 library from source. This process was time-consuming and error-prone, requiring careful attention to detail and extensive troubleshooting. For more information, please refer to the documentation provided in the GitHub repository.
\item The initial project aimed to use vibration motors as output devices, one for each direction (left, center, right). These motors would have been placed on a handlebar to provide haptic feedback to the user. However, due to time limitations and difficulties in assembling the desired prototype, the vibration motors were replaced by an LED strip for visual feedback. This change allowed for a simpler and quicker implementation while still demonstrating the core functionality of the system.
\item Limited documentation for the PyRealSense2 library made it difficult to install and utilize the camera effectively across different operating systems.
\item Debugging multi-threaded applications proved to be, as expected, a complex task. Ensuring that all threads operated correctly and efficiently required careful design, testing, and the implementation of comprehensive logging and monitoring systems.
\item Synchronizing audio and video data streams accurately to ensure that the output feedback corresponded correctly to the sensory input, particularly given the different capture rates and processing times of each modality.
\item Saturating the Arduino communication buffer when sending commands too rapidly, which led to unresponsive output devices. To address this issue, a monitoring script (\texttt{monitor\_serial.py}) was implemented to visualize the commands being sent to the Arduino in real-time, allowing for better debugging and optimization of the communication process.
\end{enumerate}

\subsection{Ideas for Future Work}

As this project was developed within a limited timeframe, there are several areas of improvement and expansion that could be explored in future work. Some potential directions for future development include:

\begin{enumerate}
\item Enhancing user interaction by implementing a user interface that allows for real-time adjustment of system parameters, such as the weighted averaging coefficients between audio and video inputs, sensitivity thresholds, and output modes.
\item Integrating the originally intended vibration motors to provide haptic feedback and evaluating their effectiveness compared to the visual feedback provided by the LED strip. This modification would not require significant changes to the existing codebase, as the communication protocol with the Arduino would remain the same.
\item Optimizing the audio processing module to increase the data processing rate beyond the current 6 items per second, allowing for more frequent updates and smoother feedback. This could involve implementing more efficient FFT algorithms or utilizing hardware acceleration.
\item Conducting real-world testing with visually impaired users to evaluate the system's performance in various environments and scenarios, providing valuable insights into its practical usability, effectiveness, and user acceptance. Such studies would also help identify necessary adjustments to the sensitivity settings and feedback mechanisms.
\item Implementing adaptive sensitivity that automatically adjusts based on environmental conditions and user preferences learned over time.
\end{enumerate}

%%
%% Conclusion

\section{Conclusion}

This project successfully developed a multimodal assistive system that integrates audio and video sensors and provides real-time feedback to the user through a set of output devices. The system architecture, based on a producer-consumer model with multi-threading and sophisticated queue management, effectively handles the complexities of data acquisition, time-sensitive processing, temporal synchronization, and communication. The implementation includes comprehensive modules for audio and video processing, intensity calculation using weighted averaging, and command generation following a well-defined protocol.

As with all prototype projects, there are several areas for improvement that could be explored in future work, including enhancing user interaction through adaptive interfaces, optimizing processing modules for higher throughput, integrating the intended haptic feedback hardware, and conducting real-world evaluations with target users. Overall, this project demonstrates the feasibility and potential of multimodal systems for enhancing mobility and safety for visually impaired individuals through intelligent sensory feedback.

%%
%% Open Science

\section*{Open Science}

The complete code for this project is available on GitHub at the following repository: \url{https://github.com/fstormacq/Master1-IIA-project}

%%
%% Acknowledgements

\begin{acks}
    This work was supported by the University of Namur, the Namur Digital Institute (NaDI), and the Research Center on Information Systems Engineering (PReCISE). The authors would like to thank all individuals who contributed to this project. A particular thanks to the providers of the hardware components that made this research possible.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{../template/bibliography}

% ===================================================================
% TODO: TO BE COMPLETED - Document ends here
% ===================================================================

\end{document}
\endinput
