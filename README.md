# Embodied and Augmented Interfaces

**Project 2025-2026**

---

## Project Description

This project aims to develop a multimodal assistive device for visually impaired individuals to enhance urban navigation safety by detecting nearby obstacles and providing comprehensive feedback through multiple sensory channels.

### Prototype Overview

The device will detect close obstacles and provide understandable multimodal feedback including:
- **Proportional vibration** based on distance
- **Directional audio** cues
- **Minimal visual indicators** when relevant

### Hardware Requirements

**Minimal setup:**
- Distance sensor (LiDAR/ToF)
- Microcontroller/mini-computer
- Integrated vibrator (cane/bracelet)
- Headphones or small speaker
- Optional: smart glasses/smartphone for visual indicators

### Multimodal Feedback System

- **Haptic feedback**: 
  - Vibration intensity proportional to obstacle distance
  - Special vibration patterns for urgent danger alerts
- **Audio feedback**: 
  - Directional sound cues
  - Tone variations to indicate priority levels
- **Visual feedback** (optional):
  - Minimal visual indicators on smart glasses or smartphone

### Target Use Case

**Urban navigation assistance for visually impaired individuals**

The system integrates with a cane and optional smart glasses to detect environmental obstacles and hazards, providing real-time multimodal alerts to enhance mobility and safety in urban environments.

## Team Members

- Nathan Lambrechts
- Edwyn Eben
- Louca Mathieu
- Simon Dourte
- Florian Stormacq

## Getting Started

*Documentation to be added as development progresses*

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.